{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanvelc/module4/blob/main/M4_AST_05_Dask_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "temporal-consequence",
      "metadata": {
        "id": "temporal-consequence"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A program by IISc and TalentSprint\n",
        "### Assignment 5: Dask"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "imported-begin",
      "metadata": {
        "id": "imported-begin"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "solid-upset",
      "metadata": {
        "id": "solid-upset"
      },
      "source": [
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* know and implement dask related libraries\n",
        "* understand and visualize parallel processing using dask delayed\n",
        "* implement dask arrays\n",
        "* handle large and complex datasets in dask dataframes\n",
        "* implement list using dask bags\n",
        "* implement Linear Regression using dask-ml"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Mc_TIXSUPf8V",
      "metadata": {
        "id": "Mc_TIXSUPf8V"
      },
      "source": [
        "## Information"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "external-gathering",
      "metadata": {
        "id": "external-gathering"
      },
      "source": [
        "### Dask\n",
        "\n",
        "Dask was launched in late 2014 by Matthew Rocklin with aims to bring native scalability to the Python Open Data Science  and overcome its single-machine restrictions. Dask consists of several different components and APIs, which can be categorized into three layers:\n",
        "\n",
        "* the scheduler\n",
        "* low-level APIs\n",
        "* high-level APIs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EKabdQH8PFyP",
      "metadata": {
        "id": "EKabdQH8PFyP"
      },
      "source": [
        "![Dask_Overview](https://cdn.iisc.talentsprint.com/CDS/Images/Dask_Overview.JPG)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "novel-franchise",
      "metadata": {
        "id": "novel-franchise"
      },
      "source": [
        "Dask is a free and open-source library for parallel computing in Python. It helps you scale your data science and machine learning workflows. It makes it easy to work with Numpy, Pandas, and Scikit-Learn, but that’s just the beginning. Dask exposes low-level APIs to its internal task scheduler to execute advanced computations. This enables building a personalised parallel computing system that uses the same engine that powers Dask’s Arrays, DataFrames, and machine learning algorithms.\n",
        "\n",
        "If you have larger-than-memory data, you can use Dask to scale up your workflow to leverage all the cores of your local workstation or even scale out to the cloud.\n",
        "\n",
        "Dask emphasizes the following virtues:\n",
        "\n",
        "* The ability to work in parallel with  NumPy array and Pandas DataFrame objects\n",
        "* Integration with other projects.\n",
        "* Distributed computing\n",
        "* Faster operation because of its low overhead and minimum serialisation\n",
        "* It runs resiliently on clusters with thousands of cores\n",
        "* Real-time feedback and diagnostics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "l_JB9kotQaHb",
      "metadata": {
        "id": "l_JB9kotQaHb"
      },
      "source": [
        "### Parallel Processing\n",
        "\n",
        "Parallel processing refers to executing multiple tasks at the same time, using multiple processors in the same machine.\n",
        "\n",
        "Generally, the code is executed in sequence, one task at a time. However, if you have a complex code that takes a long time to run but has no logic dependency on each other then instead of waiting for the previous task to complete, we compute multiple steps simultaneously. This lets you take advantage of the available processing power, which is the case in most modern computers, thereby reducing the total time taken.\n",
        "\n",
        "To learn more about dask, follow the link [here](https://docs.dask.org/en/latest/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8rxxeAgzJJM5",
      "metadata": {
        "id": "8rxxeAgzJJM5",
        "cellView": "form",
        "outputId": "287b4387-0551-4e23-d0c6-d6ea5d648597",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.2/242.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.16 requires numpy>=1.24, but you have numpy 1.23.4 which is incompatible.\n",
            "albumentations 1.4.15 requires numpy>=1.24.4, but you have numpy 1.23.4 which is incompatible.\n",
            "bigframes 1.18.0 requires numpy>=1.24.0, but you have numpy 1.23.4 which is incompatible.\n",
            "chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.23.4 which is incompatible.\n",
            "jax 0.4.33 requires numpy>=1.24, but you have numpy 1.23.4 which is incompatible.\n",
            "jaxlib 0.4.33 requires numpy>=1.24, but you have numpy 1.23.4 which is incompatible.\n",
            "pandas-stubs 2.1.4.231227 requires numpy>=1.26.0; python_version < \"3.13\", but you have numpy 1.23.4 which is incompatible.\n",
            "tensorflow 2.17.0 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 1.23.4 which is incompatible.\n",
            "xarray 2024.9.0 requires numpy>=1.24, but you have numpy 1.23.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.8/149.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.3/237.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi\n",
            "  Downloading fastapi-0.115.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi)\n",
            "  Downloading starlette-0.38.6-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.9.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.23.4)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.39.0,>=0.37.2->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.39.0,>=0.37.2->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.39.0,>=0.37.2->fastapi) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.39.0,>=0.37.2->fastapi) (1.2.2)\n",
            "Downloading fastapi-0.115.0-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.38.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: starlette, fastapi\n",
            "Successfully installed fastapi-0.115.0 starlette-0.38.6\n",
            "Collecting kaleido\n",
            "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl.metadata (15 kB)\n",
            "Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kaleido\n",
            "Successfully installed kaleido-0.2.1\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.31.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (4.12.2)\n",
            "Downloading uvicorn-0.31.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, uvicorn\n",
            "Successfully installed h11-0.14.0 uvicorn-0.31.0\n",
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.11-py3-none-any.whl.metadata (1.9 kB)\n",
            "Downloading python_multipart-0.0.11-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: python-multipart\n",
            "Successfully installed python-multipart-0.0.11\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires numpy<2.0a0,>=1.23, but you have numpy 2.1.1 which is incompatible.\n",
            "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.1.1 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.1.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.1 which is incompatible.\n",
            "pandas 2.1.4 requires numpy<2,>=1.22.4; python_version < \"3.11\", but you have numpy 2.1.1 which is incompatible.\n",
            "pytensor 2.25.4 requires numpy<2,>=1.17.0, but you have numpy 2.1.1 which is incompatible.\n",
            "rmm-cu12 24.4.0 requires numpy<2.0a0,>=1.23, but you have numpy 2.1.1 which is incompatible.\n",
            "tensorflow 2.17.0 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.1.1 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (8.5.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata) (3.20.2)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "graphviz is already the newest version (2.42.2-6ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (0.20.3)\n"
          ]
        }
      ],
      "source": [
        "# @title Install  the Required Dependensies and Restart the Runtime\n",
        "!pip -qq install dask[complete]\n",
        "!pip -qq install dask distributed\n",
        "!pip -qq install mimesis==7.0.0\n",
        "!pip -qq install dask\n",
        "# Uninstalling Numpy 1.24.3 as 'dask_ml' requires numpy<1.24.0, >=1.18.0\n",
        "!pip -qq uninstall numpy -y\n",
        "# jax>=0.4.8 is necessarry\n",
        "!pip -qq install numpy==1.23.4\n",
        "!pip -qq install dask-ml\n",
        "!pip install fastapi\n",
        "!pip install kaleido\n",
        "!pip install uvicorn\n",
        "!pip install python-multipart\n",
        "!pip -qq install jax>=0.4.9\n",
        "!pip install --upgrade importlib-metadata\n",
        "!apt-get install -y graphviz\n",
        "!pip install graphviz"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fRUmEMOCv-d2",
      "metadata": {
        "id": "fRUmEMOCv-d2"
      },
      "source": [
        "**Note: After running above cell, we will need to `Restart Runtime` to successfully import dask_ml**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8ZyR1qqtr6_",
      "metadata": {
        "id": "b8ZyR1qqtr6_"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "qXDbGPh1tyNR",
      "metadata": {
        "id": "qXDbGPh1tyNR"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2306024\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6SARMN_-t3ny",
      "metadata": {
        "id": "6SARMN_-t3ny"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"9742221781\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "Bkw9Am4ht93r",
      "metadata": {
        "id": "Bkw9Am4ht93r",
        "cellView": "form",
        "outputId": "d146899e-d8cc-4c98-8a83-e2ac953612ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2306024&recordId=4996\"></script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ],
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M4_AST_05_Dask_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "    ipython.magic(\"sx pip -qq install dask[complete]\")\n",
        "    ipython.magic(\"sx pip -qq install dask distributed\")\n",
        "    ipython.magic(\"sx pip -qq install mimesis\")\n",
        "    ipython.magic(\"sx pip -qq install dask\")\n",
        "    #ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/CDS/MiniProjects/BlackFriday.zip\")\n",
        "    #ipython.magic(\"sx unzip BlackFriday.zip\")\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/CDS/Datasets/nycflights.zip\")\n",
        "    ipython.magic(\"sx unzip nycflights.zip\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://cds-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "amber-breast",
      "metadata": {
        "id": "amber-breast"
      },
      "source": [
        "#### Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy<2.0"
      ],
      "metadata": {
        "id": "erSPK273sP3d",
        "outputId": "48c8cffd-2bd6-43f6-af94-5c40ab180447",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "erSPK273sP3d",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: 2.0: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "marked-penny",
      "metadata": {
        "id": "marked-penny",
        "outputId": "42e115fc-5ba8-4618-c8ac-975492e0049d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.1.1 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-5-eaeda177b159>\", line 3, in <cell line: 3>\n",
            "    import dask.array as da          # for handling large arrays\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dask/array/__init__.py\", line 34, in <module>\n",
            "    from dask.array import backends, fft, lib, linalg, ma, overlap, random\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dask/array/backends.py\", line 8, in <module>\n",
            "    from dask.array.core import Array\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dask/array/core.py\", line 30, in <module>\n",
            "    from dask.array.chunk_types import is_valid_array_chunk, is_valid_chunk_type\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dask/array/chunk_types.py\", line 6, in <module>\n",
            "    _HANDLED_CHUNK_TYPES = [np.ndarray, np.ma.MaskedArray]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numpy/__init__.py\", line 346, in __getattr__\n",
            "    # Pytest testing\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numpy/ma/__init__.py\", line 42, in <module>\n",
            "    from . import core\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numpy/ma/core.py\", line 32, in <module>\n",
            "    import numpy.core.umath as umath\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numpy/core/umath.py\", line 14, in <module>\n",
            "    from ._multiarray_umath import _UFUNC_API, _add_newdoc_ufunc, _ones_like\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.1.1 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n\n\nDask array requirements are not installed.\n\nPlease either conda or pip install as follows:\n\n  conda install dask                 # either conda install\n  python -m pip install \"dask[array]\" --upgrade  # or python -m pip install",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dask/array/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackends\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverlap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shuffle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dask/array/backends.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m from dask.array.dispatch import (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dask/array/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_types\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_valid_array_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_chunk_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dask/array/chunk_types.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Start list of valid chunk types, to be added to with guarded imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0m_HANDLED_CHUNK_TYPES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m     \u001b[0;31m# Pytest testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytesttester\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPytestTester\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/ma/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \"\"\"\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/ma/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mumath\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mumath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumerictypes\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mntypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/umath.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# _ones_like is semi-public, on purpose not added to __all__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_multiarray_umath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_UFUNC_API\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_add_newdoc_ufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ones_like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/_multiarray_umath.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr_name)\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.1.1 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-eaeda177b159>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m                        \u001b[0;31m# provides interaction with operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mda\u001b[0m          \u001b[0;31m# for handling large arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdask\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdelayed\u001b[0m         \u001b[0;31m# import the delayed function of dask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdd\u001b[0m      \u001b[0;31m# library to import Dask DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dask/array/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;34m'  python -m pip install \"dask[array]\" --upgrade  # or python -m pip install'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.1.1 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n\n\nDask array requirements are not installed.\n\nPlease either conda or pip install as follows:\n\n  conda install dask                 # either conda install\n  python -m pip install \"dask[array]\" --upgrade  # or python -m pip install",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os                        # provides interaction with operating system\n",
        "import dask\n",
        "import dask.array as da          # for handling large arrays\n",
        "from dask import delayed         # import the delayed function of dask\n",
        "import dask.dataframe as dd      # library to import Dask DataFrame\n",
        "import dask.bag as db            # import the bag from dask\n",
        "from dask_ml.linear_model import LinearRegression\n",
        "import json                      # importing JSON files\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from time import sleep"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "disciplinary-merchant",
      "metadata": {
        "id": "disciplinary-merchant"
      },
      "source": [
        "**Implementation of Parallel Processing with Dask (Basic Idea)**\n",
        "\n",
        "Here, we can compare the normal python function with dask delayed function at the time of execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "interstate-excuse",
      "metadata": {
        "id": "interstate-excuse"
      },
      "outputs": [],
      "source": [
        "# define apply_discount function and pass a variable x\n",
        "def apply_discount(x):\n",
        "  sleep(1)\n",
        "  # discount of 20 %\n",
        "  x=x-0.2*x\n",
        "  return x\n",
        "\n",
        "# define two variables a, b and add them\n",
        "def get_total(a,b):\n",
        "  sleep(1)\n",
        "  return a+b\n",
        "\n",
        "# define function for total price of items\n",
        "def get_total_price(x,y):\n",
        "  sleep(1)\n",
        "  a=apply_discount(x)\n",
        "  b=apply_discount(y)\n",
        "  get_total(a,b)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fifth-lightweight",
      "metadata": {
        "id": "fifth-lightweight"
      },
      "source": [
        "Given a number, the above code simply applies a 20 percent discount on price and then adds them. Inserted a sleep function explicitly, both the functions take 1 sec to execute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "polished-mailing",
      "metadata": {
        "id": "polished-mailing",
        "outputId": "d6878df5-8303-4d85-b625-6c427556ec2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'sleep' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-b78a6a47bb7a>\u001b[0m in \u001b[0;36mapply_discount\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# define apply_discount function and pass a variable x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mapply_discount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;31m# discount of 20 %\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sleep' is not defined"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# apply discount of 20 % on 100\n",
        "x = apply_discount(100)\n",
        "# apply a discount of 20 % on 200\n",
        "y = apply_discount(200)\n",
        "# add a and b\n",
        "z = get_total_price(x,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "monthly-chocolate",
      "metadata": {
        "id": "monthly-chocolate"
      },
      "source": [
        "We have recorded the time taken for this execution using %%time as shown. You can observe that time taken is ~6.01 seconds when it is executed sequentially. Now, let’s see how to use dask.delayed to reduce this time."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hispanic-newport",
      "metadata": {
        "id": "hispanic-newport"
      },
      "source": [
        "Now, you can transform the functions apply_discount() and get_total_price(). You can use delayed() function to wrap the function calls that you want to turn into tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "moved-deviation",
      "metadata": {
        "id": "moved-deviation"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Wrapping all the function calls using dask.delayed\n",
        "x = delayed(apply_discount)(100)\n",
        "y = delayed(apply_discount)(200)\n",
        "z = delayed(get_total_price)(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "heavy-compatibility",
      "metadata": {
        "id": "heavy-compatibility"
      },
      "source": [
        "**What does dask.delayed do?**\n",
        "\n",
        "The Dask delayed function decorates your functions so that they operate lazily. Rather than executing your function immediately, it will defer execution, placing the function and its arguments into a task graph. It wraps a function or object to produce a Delayed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "compact-fortune",
      "metadata": {
        "id": "compact-fortune"
      },
      "outputs": [],
      "source": [
        "# visualize in graphical way\n",
        "z.visualize()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "serious-message",
      "metadata": {
        "id": "serious-message"
      },
      "source": [
        "Clearly, from the above image, you can see there are two instances of apply_discount() function called in parallel. This is an opportunity to save time and processing power by executing them simultaneously.\n",
        "\n",
        "Till now, the task graph is computed. To actually execute it, let’s call the compute() method of z."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "loaded-radar",
      "metadata": {
        "id": "loaded-radar"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# compute() is used to show the result in dask\n",
        "z.compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "surrounded-vitamin",
      "metadata": {
        "id": "surrounded-vitamin"
      },
      "source": [
        "**Note**: When we create an object, it is just a blueprint until you call `compute()`. That is when the job gets distributed to all the workers, and the actual function gets called, or concrete values are generated."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "instructional-edinburgh",
      "metadata": {
        "id": "instructional-edinburgh"
      },
      "source": [
        "The total time taken has been reduced. This is the basic concept of parallel computing. Dask makes it very convenient.\n",
        "\n",
        "To know more about delayed function, click [here](https://docs.dask.org/en/latest/delayed.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XK77RalhRLVX",
      "metadata": {
        "id": "XK77RalhRLVX"
      },
      "source": [
        "### Arrays\n",
        "\n",
        "![dask_array](https://cdn.iisc.talentsprint.com/CDS/Images/Dask_array.JPG)\n",
        "\n",
        "Dask array provides a parallel, larger-than-memory, n-dimensional array using blocked algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "refined-wisconsin",
      "metadata": {
        "id": "refined-wisconsin"
      },
      "source": [
        "* Parallel: Uses all of the cores on your computer\n",
        "\n",
        "* Larger-than-memory: Lets you work on datasets that are larger than your available memory by breaking up your array into many small pieces, operating on those pieces in an order that minimizes the memory footprint of your computation, and effectively streaming data from disk.\n",
        "\n",
        "* Blocked Algorithms: Performs large computations by performing many smaller computations\n",
        "\n",
        "\n",
        "In the below example, dask arrays and normal numpy arrays are used and compared based on the time taken for execution.\n",
        "\n",
        "1. Construct a 40000x40000 array of normally distributed random values broken up into 1000x1000 sized chunks\n",
        "\n",
        "2. Take the mean along one axis\n",
        "\n",
        "3. Take every 100th element\n",
        "\n",
        "To know more about dask arrays, click [here](https://docs.dask.org/en/latest/array.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "insured-shaft",
      "metadata": {
        "id": "insured-shaft"
      },
      "source": [
        "Creating an array with random normal distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "antique-strengthening",
      "metadata": {
        "id": "antique-strengthening"
      },
      "outputs": [],
      "source": [
        "# creating a random normal dask array with a mean value of 20 and sigma value of 0.2\n",
        "# 1600 million element array with 1000 by 1000 chunks\n",
        "x = da.random.normal(20, 0.2, size=(40000, 40000),\n",
        "                              chunks=(1000, 1000))   # Cut into 1000x1000 sized chunks\n",
        "# Perform NumPy-style operations to calculate mean along the rows (axis=0)\n",
        "y = x.mean(axis=0)[::100]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "original-orbit",
      "metadata": {
        "id": "original-orbit"
      },
      "source": [
        "Computing the mean of x and capturing the time using the magical command"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WDn-xXVz9kCq",
      "metadata": {
        "id": "WDn-xXVz9kCq"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# computing the mean of x\n",
        "y.compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "virtual-communications",
      "metadata": {
        "id": "virtual-communications"
      },
      "source": [
        "Dask.array is a NumPy-like library that operates on large datasets that don’t fit into memory. It extends beyond the linear problems to full N-Dimensional algorithms and a decent subset of the NumPy interface.\n",
        "\n",
        "Create ``dask.array`` object\n",
        "\n",
        "We can create a `dask.array` Array object with the `da.from_array` function. This function accepts\n",
        "\n",
        "1. data: Any object that supports NumPy slicing\n",
        "\n",
        "2. [chunks](https://docs.dask.org/en/latest/array-chunks.html): A chunk size defines the size of the numpy arrays\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "white-plane",
      "metadata": {
        "id": "white-plane"
      },
      "source": [
        "**Manipulate ``dask.array`` object as a numpy array**\n",
        "\n",
        "Now we have an Array and we perform standard numpy-style computations that include arithmetic, mathematics, slicing, reductions, etc.\n",
        "\n",
        "The interface is familiar, but the actual work is different. dask_array.sum() does not do the same thing as numpy_array.sum().\n",
        "\n",
        "**What’s the difference?**\n",
        "\n",
        "dask_array.sum() builds an expression of the computation. It does not do the computation yet. numpy_array.sum() computes the sum immediately.\n",
        "\n",
        "*Why the difference?*\n",
        "\n",
        "Dask arrays are split into chunks. Each chunk must have computations run on that chunk explicitly. If the desired answer comes from a small slice of the entire dataset, running the computation overall data would be wasteful of CPU and memory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "precise-barcelona",
      "metadata": {
        "id": "precise-barcelona"
      },
      "source": [
        "#### Performance comparison\n",
        "\n",
        "Let's create a random numpy array of normal distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mobile-triple",
      "metadata": {
        "id": "mobile-triple"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# create a normal distribution array with 400 million element using numpy\n",
        "x = np.random.normal(20, 0.2, size=(20000, 20000))\n",
        "# calculate the mean\n",
        "y = x.mean(axis=0)[::100]\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "capital-villa",
      "metadata": {
        "id": "capital-villa"
      },
      "source": [
        "\n",
        "Now Let's create a random dask array with normal distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "involved-festival",
      "metadata": {
        "id": "involved-festival"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# created the same array as above and made chunks of size 1000x1000\n",
        "x = da.random.normal(20, 0.2, size=(20000, 20000), chunks=(1000, 1000))\n",
        "# calculate the mean\n",
        "y = x.mean(axis=0)[::100]\n",
        "y.compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "found-reporter",
      "metadata": {
        "id": "found-reporter"
      },
      "source": [
        "Notice that the Dask array computation ran in ~13 seconds but used ~25 seconds of user CPU time. The numpy computation ran in ~19 seconds and used ~18 seconds of user CPU time.\n",
        "\n",
        "Dask finished faster but used more total CPU time because Dask was able to transparently parallelize the computation because of the chunk size."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0aDsiToL_G8b",
      "metadata": {
        "id": "0aDsiToL_G8b"
      },
      "source": [
        "Let us study another example on dask arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5XRgC93P-4S_",
      "metadata": {
        "id": "5XRgC93P-4S_"
      },
      "outputs": [],
      "source": [
        "# define the shape of the array\n",
        "shape = (1000, 4000)\n",
        "ones_np = np.ones(shape)\n",
        "ones_np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EwCEJrZH_DnO",
      "metadata": {
        "id": "EwCEJrZH_DnO"
      },
      "source": [
        "This array contains exactly 32 MB of data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CNcUXl30_XV_",
      "metadata": {
        "id": "CNcUXl30_XV_"
      },
      "outputs": [],
      "source": [
        "# print the memory of array created\n",
        "print('%.1f MB' % (ones_np.nbytes / 1e6))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oHJCX0Jq_ldt",
      "metadata": {
        "id": "oHJCX0Jq_ldt"
      },
      "source": [
        "Now let’s create the same array using dask’s array interface.\n",
        "\n",
        "A crucial difference with dask is that we must specify the chunks argument. “Chunks” describes how the array is split up over many sub-arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2vMwTmHo_shW",
      "metadata": {
        "id": "2vMwTmHo_shW"
      },
      "outputs": [],
      "source": [
        "# defining chunk in dask array\n",
        "chunk_shape = (1000, 1000)\n",
        "ones = da.ones(shape, chunks=chunk_shape)\n",
        "ones"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JOvkTZkU_-Nr",
      "metadata": {
        "id": "JOvkTZkU_-Nr"
      },
      "source": [
        "Notice that we just see a symbolic representation of the array, including its shape, dtype, and chunk size. No data has been generated yet. When we call .compute() on a dask array, the computation is triggered, and the dask array becomes a numpy array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1Ecr38LuAFfQ",
      "metadata": {
        "id": "1Ecr38LuAFfQ"
      },
      "outputs": [],
      "source": [
        "ones.compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DAl1yL0jAHYO",
      "metadata": {
        "id": "DAl1yL0jAHYO"
      },
      "source": [
        "In order to understand what happened when we called .compute(), we can visualize the dask graph, the symbolic operations that make up the array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ABGAp0ohAKtP",
      "metadata": {
        "id": "ABGAp0ohAKtP"
      },
      "outputs": [],
      "source": [
        "ones.visualize(format='svg')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8p-P_I4MAOYA",
      "metadata": {
        "id": "8p-P_I4MAOYA"
      },
      "source": [
        "Our array has four chunks. To generate it, dask calls np.ones four times and then concatenates this together into one array.\n",
        "\n",
        "Rather than immediately loading a dask array (which puts all the data into RAM), it is more common to reduce the data somehow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7wogfk0WASfV",
      "metadata": {
        "id": "7wogfk0WASfV"
      },
      "outputs": [],
      "source": [
        "# calculate the sum of elements of dask array\n",
        "sum_of_ones = ones.sum()\n",
        "# visualize the sum operation in dask\n",
        "sum_of_ones.visualize(format='svg')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "urIQVP-xQ9XO",
      "metadata": {
        "id": "urIQVP-xQ9XO"
      },
      "source": [
        "### DataFrame\n",
        "\n",
        "A Dask DataFrame is a large parallel DataFrame composed of many smaller Pandas DataFrames, split along **with** the index. One Dask DataFrame operation triggers many operations on the constituent Pandas DataFrames.\n",
        "\n",
        "Dask DataFrames coordinate many Pandas DataFrames/Series arranged along the index. A Dask DataFrame is partitioned row-wise, grouping rows by index value for efficiency.\n",
        "\n",
        "![Dask_DF](https://cdn.iisc.talentsprint.com/CDS/Images/Dask_Dataframe.JPG)\n",
        "\n",
        "To study more about Dask dataframe, refer to the link [here](https://docs.dask.org/en/latest/dataframe.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Zyw-S0i7QQ_U",
      "metadata": {
        "id": "Zyw-S0i7QQ_U"
      },
      "source": [
        "Create a dataframe with random values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H4xmg4gHQdTo",
      "metadata": {
        "id": "H4xmg4gHQdTo"
      },
      "outputs": [],
      "source": [
        "# pandas and numpy is used to create dataframe of random values\n",
        "data = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81SudTyQSD8a",
      "metadata": {
        "id": "81SudTyQSD8a"
      },
      "source": [
        "Dataframe is created with two columns, each having 1500 random values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4iu2Pps_Erfi",
      "metadata": {
        "id": "4iu2Pps_Erfi"
      },
      "outputs": [],
      "source": [
        "# create two columns\n",
        "data['col1'] = np.random.normal(size=1500)\n",
        "data['col2'] = np.random.normal(size=1500)\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8zEcwuXZSM9A",
      "metadata": {
        "id": "8zEcwuXZSM9A"
      },
      "source": [
        "Read the dataframe using dask dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2LFQep8Q1ga",
      "metadata": {
        "id": "f2LFQep8Q1ga"
      },
      "outputs": [],
      "source": [
        "# convert pandas dataframe to dask dataframe\n",
        "ddf = dd.from_pandas(data, npartitions=2)\n",
        "print(ddf.compute())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eqtBj6xsRBla",
      "metadata": {
        "id": "eqtBj6xsRBla"
      },
      "outputs": [],
      "source": [
        "# Get a dask DataFrame/Series representing the nth partition\n",
        "part_1= ddf.get_partition(1)\n",
        "print(part_1.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5QyY3Hu1RlWR",
      "metadata": {
        "id": "5QyY3Hu1RlWR"
      },
      "outputs": [],
      "source": [
        "%time ddf['col1'].value_counts().compute()                 # compute the value counts in column 1 and time of execution of code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PwfxN8zIyZ1V",
      "metadata": {
        "id": "PwfxN8zIyZ1V"
      },
      "source": [
        "So, using dask dataframe it is much more time efficient to execute"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "another-warren",
      "metadata": {
        "id": "another-warren"
      },
      "source": [
        "#### Performance of Dask Dataframe and Pandas Dataframe\n",
        "\n",
        "\n",
        "\n",
        "In the below example, we can understand how dask dataframes handle complex and large datasets along with pandas dataframe.\n",
        "\n",
        "We are using 'nycflights' dataset to show how dask differs from pandas.\n",
        "\n",
        "'nycflights' dataset contains 10 separate csv flights for the year 1990 to 1999."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vkIiWr265y1k",
      "metadata": {
        "id": "vkIiWr265y1k"
      },
      "outputs": [],
      "source": [
        "nycflightsPath = os.path.join('nycflights', 'nycflights', '*.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TtpYoVlZbaUE",
      "metadata": {
        "id": "TtpYoVlZbaUE"
      },
      "outputs": [],
      "source": [
        "nycflightsPath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "finished-mouse",
      "metadata": {
        "id": "finished-mouse"
      },
      "outputs": [],
      "source": [
        "# reading all the csv file using dask\n",
        "df_dd = dd.read_csv(nycflightsPath,\n",
        "                 parse_dates={'Date':[0,1,2]},\n",
        "                 dtype={'CRSElapsedTime': 'float64',\n",
        "                        'TailNum': 'object',\n",
        "                        'Cancelled': 'bool'})\n",
        "\n",
        "df_dd.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "weighted-panic",
      "metadata": {
        "id": "weighted-panic"
      },
      "source": [
        "Here, we can see that dask dataframe stores all the csv files simultaneously using *.csv (this code reads all the files inside nycflights dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fantastic-orientation",
      "metadata": {
        "id": "fantastic-orientation"
      },
      "source": [
        "Whereas, when we want to store the files using pandas, it leads to errors. This means pandas is not able to handle and store more than one file simultaneously. So, the following code cell will give an error.\n",
        "\n",
        "Here, in pandas, we need to store different csv files using separate commands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "collectible-swing",
      "metadata": {
        "id": "collectible-swing"
      },
      "outputs": [],
      "source": [
        "# this gives an error\n",
        "try:\n",
        "  # using pandas trying to read all csv files\n",
        "  df_pd = pd.read_csv(nycfllightsPath, parse_dates={'Date':[0,1,2]})\n",
        "except:\n",
        "  print(\"Error Occured!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5msBx5FO680B",
      "metadata": {
        "id": "5msBx5FO680B"
      },
      "source": [
        "Using pandas we can read one single dataset at a time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Zd9OneVOb3z-",
      "metadata": {
        "id": "Zd9OneVOb3z-"
      },
      "outputs": [],
      "source": [
        "!head /content/nycflights/nycflights/1990.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "social-harbor",
      "metadata": {
        "id": "social-harbor"
      },
      "outputs": [],
      "source": [
        "# reading only single csv file\n",
        "nycf_1990 = pd.read_csv('/content/nycflights/nycflights/1990.csv', parse_dates={'Date':[0,1,2]})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "conventional-supplement",
      "metadata": {
        "id": "conventional-supplement"
      },
      "source": [
        "Dask dataframes object has no data, unlike the pandas data frame. Dask has just read the start of the first file and the column names and dtypes. That's why dask is considered to be lazy in operation.\n",
        "\n",
        "For seeing the data in the dask dataframe we need to execute .head() function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "realistic-palestine",
      "metadata": {
        "id": "realistic-palestine"
      },
      "outputs": [],
      "source": [
        "# first five rows of the dataset\n",
        "df_dd.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "658CIRFxcLUc",
      "metadata": {
        "id": "658CIRFxcLUc"
      },
      "outputs": [],
      "source": [
        "df_dd.compute().shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "parental-framework",
      "metadata": {
        "id": "parental-framework"
      },
      "source": [
        "#### Computations With Dask Dataframes\n",
        "\n",
        "We want to compute the maximum value of *DepDelay* (the delays occurring in the departure of flights). Now if we are working with simple pandas on this dataset than we need to compute the max. value in each individual data file and concatenate these 10 values together which gives us the required max. value of the whole dataset and this may be put as out of memory.\n",
        "\n",
        "But in dask dataframe we can handle partition more efficiently.\n",
        "\n",
        "dask.dataframe lets us write pandas-like code, that operates on larger than memory datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the CSV file using Dask, assuming missing data and avoiding blocksize to handle large datasets\n",
        "df_dd = dd.read_csv('/content/nycflights/nycflights/1990.csv', assume_missing=True, blocksize=None)\n",
        "\n",
        "# Inspect the data to verify it is loaded correctly\n",
        "print(df_dd.head())"
      ],
      "metadata": {
        "id": "ZnDferu9nMJE"
      },
      "id": "ZnDferu9nMJE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the maximum departure delay\n",
        "%time max_dep_delay = df_dd['DepDelay'].max().compute() # compute() is used in order to get the required value\n",
        "print(f\"Maximum Departure Delay: {max_dep_delay}\")"
      ],
      "metadata": {
        "id": "VpJvlhAsnqoj"
      },
      "id": "VpJvlhAsnqoj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "smoking-little",
      "metadata": {
        "id": "smoking-little"
      },
      "source": [
        "Above code has gone through approximately 2 million entries.\n",
        "As with Delayed objects, we can go through the underlying graphs using `visualize()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "equipped-concord",
      "metadata": {
        "id": "equipped-concord"
      },
      "outputs": [],
      "source": [
        "# visualize the above code using structure diagram\n",
        "df_dd.DepDelay.max().visualize(rankdir='LR', size='15,15!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dried-catalog",
      "metadata": {
        "id": "dried-catalog"
      },
      "source": [
        "### Dask Bag"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "described-coupon",
      "metadata": {
        "id": "described-coupon"
      },
      "source": [
        "Dask Bag implements operations like map, filter, fold, and groupby on collections of generic Python objects. It does this in parallel with a small memory footprint using Python iterators. It is similar to a parallel version of PyToolz or a Pythonic version of the PySpark RDD.\n",
        "\n",
        "Dask bags coordinate many Python lists or Iterators, each of which forms a partition of a larger collection."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "infrared-accreditation",
      "metadata": {
        "id": "infrared-accreditation"
      },
      "source": [
        "Execution on bags provide two benefits:\n",
        "\n",
        "1. Parallel: data is split up, allowing multiple cores or machines to execute in parallel\n",
        "2. Iterating: data processes lazily, allowing smooth execution of larger-than-memory data, even on a single machine within a single partition\n",
        "\n",
        "Bag is the mathematical name for an unordered collection allowing repeats. It uses Parallel lists for semi-structured data.\n",
        "\n",
        "DataFrames are limited to only two dimensions (rows and columns), but Arrays can have many more, whereas Bag is helpful dealing with JSON blobs or text data (unstructured data).\n",
        "\n",
        "To study dask bags, refer the link [here](https://docs.dask.org/en/latest/bag.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uXV24wC5Y2bi",
      "metadata": {
        "id": "uXV24wC5Y2bi"
      },
      "outputs": [],
      "source": [
        "# creating a bag\n",
        "bag1 = db.from_sequence(range(1000000), partition_size=1000, npartitions=1000)\n",
        "bag1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JRdL0r2qY--y",
      "metadata": {
        "id": "JRdL0r2qY--y"
      },
      "outputs": [],
      "source": [
        "final_bag1 = bag1.filter(lambda x: x%100 == 0)\n",
        "\n",
        "len(final_bag1.compute())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "X4STVdbkabK4",
      "metadata": {
        "id": "X4STVdbkabK4"
      },
      "source": [
        "Advantage of using bags with the help of `groupby`\n",
        "\n",
        "we are looping through a list of values (tuple). We want to take a sum of the second value in tuples where the first value is the same. We have both normal python and dask.bag API version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fQpzElEsaZkf",
      "metadata": {
        "id": "fQpzElEsaZkf"
      },
      "outputs": [],
      "source": [
        "# Using python\n",
        "x = [(\"a\",100), (\"b\",200), (\"c\",300), (\"d\",400), (\"e\",500), (\"a\",200), (\"e\",300)]\n",
        "\n",
        "# Empty dictionary\n",
        "result = {}\n",
        "\n",
        "# Iterate the list for each item as tuple\n",
        "for key, val in x:\n",
        "    # sum-up the value if key already exist\n",
        "    if key in result:\n",
        "        result[key] += val\n",
        "    # create a key-value pair\n",
        "    else:\n",
        "        result[key] = val\n",
        "\n",
        "list(result.items())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zEF_UzBAbQ-K",
      "metadata": {
        "id": "zEF_UzBAbQ-K"
      },
      "source": [
        "Now let's do the same with bags in a simple way using `groupby`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Vdh-gNEsa2O6",
      "metadata": {
        "id": "Vdh-gNEsa2O6"
      },
      "outputs": [],
      "source": [
        "# using bags API\n",
        "bag1 = db.from_sequence([(\"a\",100), (\"b\",200), (\"c\",300), (\"d\",400), (\"e\",500), (\"a\",200), (\"e\",300)])\n",
        "\n",
        "# using groupby\n",
        "bag1.groupby(lambda x: x[0]).map(lambda x: (x[0], sum([i[1] for i in x[1]]))).compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "white-hunter",
      "metadata": {
        "id": "white-hunter"
      },
      "source": [
        "#### Semi-Structured Data\n",
        "\n",
        "We create a random set of record data and store it to disk as many JSON files. This will serve as our data for this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81uRKZU7B_4r",
      "metadata": {
        "id": "81uRKZU7B_4r"
      },
      "outputs": [],
      "source": [
        "# Create data/ directory\n",
        "os.makedirs('data', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kBglmKNjCGUy",
      "metadata": {
        "id": "kBglmKNjCGUy"
      },
      "outputs": [],
      "source": [
        "# Make records of people\n",
        "y = dask.datasets.make_people()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "forbidden-integer",
      "metadata": {
        "id": "forbidden-integer"
      },
      "outputs": [],
      "source": [
        "# Encode as JSON, write to disk\n",
        "y.map(json.dumps).to_textfiles('data/*.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "parental-behalf",
      "metadata": {
        "id": "parental-behalf"
      },
      "source": [
        "created and read the JSON files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "accessible-nurse",
      "metadata": {
        "id": "accessible-nurse"
      },
      "outputs": [],
      "source": [
        "# read the third row of the dataset\n",
        "y.take(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "swedish-raleigh",
      "metadata": {
        "id": "swedish-raleigh"
      },
      "source": [
        "**Map, Filter on Bag**\n",
        "\n",
        "We can process this data by filtering out only certain records of interest, mapping functions over it to process our data, and aggregating those results to a total value."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "english-facing",
      "metadata": {
        "id": "english-facing"
      },
      "source": [
        "Select only people over 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "blocked-centre",
      "metadata": {
        "id": "blocked-centre"
      },
      "outputs": [],
      "source": [
        "# filter the data\n",
        "y.filter(lambda record: record['age'] > 30).take(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "secure-antibody",
      "metadata": {
        "id": "secure-antibody"
      },
      "source": [
        "Select the occupation field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "preliminary-updating",
      "metadata": {
        "id": "preliminary-updating"
      },
      "outputs": [],
      "source": [
        "# Selecting particular feature\n",
        "y.map(lambda record: record['occupation']).take(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DaX9J5q6-jUu",
      "metadata": {
        "id": "DaX9J5q6-jUu"
      },
      "source": [
        " Count the total number of records"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "experienced-stocks",
      "metadata": {
        "id": "experienced-stocks"
      },
      "outputs": [],
      "source": [
        "# computing the total number of records in y\n",
        "y.count().compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ardWCXA6B2v1",
      "metadata": {
        "id": "ardWCXA6B2v1"
      },
      "source": [
        "### Machine Learning With Dask"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mAfNu9D3QNhj",
      "metadata": {
        "id": "mAfNu9D3QNhj"
      },
      "source": [
        "Dask-ML wants to enable scalable machine learning in Python. It aims to do so by\n",
        "\n",
        "1. Working with existing libraries within the Python ecosystem\n",
        "2. Using the features of Dask to scale computation to larger datasets and larger problems"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eCSCipNJZWUp",
      "metadata": {
        "id": "eCSCipNJZWUp"
      },
      "source": [
        "#### Generalized Linear Models\n",
        "\n",
        "Generalized linear models are a broad class of commonly used models. These implementations scale out well to large datasets either on a single machine or distributed cluster. They can be powered by a variety of optimization algorithms and use a variety of regularizers.\n",
        "\n",
        "These follow the scikit-learn estimator API, and so can be dropped into existing routines like grid search and pipelines, but are implemented externally with new, scalable algorithms and so can consume distributed dask arrays and dataframes rather than just single-machine NumPy and Pandas arrays and dataframes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hAHK3At99aca",
      "metadata": {
        "id": "hAHK3At99aca"
      },
      "outputs": [],
      "source": [
        "from dask_ml.datasets import make_regression\n",
        "from dask_ml.model_selection import train_test_split\n",
        "\n",
        "X, y = make_regression(n_samples=200000, n_features=100, random_state=0, chunks=10000)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Xfm-6x5u-j2X",
      "metadata": {
        "id": "Xfm-6x5u-j2X"
      },
      "outputs": [],
      "source": [
        "# fit the model\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "lr.score(X_test,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y6U25XbACagU",
      "metadata": {
        "id": "y6U25XbACagU"
      },
      "outputs": [],
      "source": [
        "#predict on test data\n",
        "pred=lr.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CnDOZAAMDWjE",
      "metadata": {
        "id": "CnDOZAAMDWjE"
      },
      "outputs": [],
      "source": [
        "# predicted values\n",
        "pred.compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IO9hQhBXZvd2",
      "metadata": {
        "id": "IO9hQhBXZvd2"
      },
      "source": [
        "To know more about dask-ml, click [here](https://ml.dask.org/)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CFNLB7O4utb3",
      "metadata": {
        "id": "CFNLB7O4utb3"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "_38KV9uluveS",
      "metadata": {
        "id": "_38KV9uluveS"
      },
      "outputs": [],
      "source": [
        "# @title Select the FALSE statement: { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"Dask can operate in parallel only on datasets that fit into the main memory\" #@param [\"\",\"Dask can operate in parallel only on datasets that fit into the main memory\",\"Dask provides high level Array Bag and DataFrame collections that mimic NumPy lists and Pandas\",\"Dask provides dynamic task schedulers that execute task graphs in parallel\",\"Dask performs lazy execution by saving the set of inputs and executing only on demand\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "IVj6KRihwhPy",
      "metadata": {
        "id": "IVj6KRihwhPy"
      },
      "outputs": [],
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"Too Difficult for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "G7Wx_ufawlf8",
      "metadata": {
        "id": "G7Wx_ufawlf8"
      },
      "outputs": [],
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"na\" #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2Fbfkz82w5BI",
      "metadata": {
        "id": "2Fbfkz82w5BI"
      },
      "outputs": [],
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "RauzHTwWw8PW",
      "metadata": {
        "id": "RauzHTwWw8PW"
      },
      "outputs": [],
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"Somewhat Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "wHVmS408w_Si",
      "metadata": {
        "id": "wHVmS408w_Si"
      },
      "outputs": [],
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"Didn't use\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "T-LbsTp6xCy3",
      "metadata": {
        "cellView": "form",
        "id": "T-LbsTp6xCy3",
        "outputId": "52eb9f9f-baf4-4b14-a309-24e98f1c58a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your submission is successful.\n",
            "Ref Id: 4996\n",
            "Date of submission:  28 Sep 2024\n",
            "Time of submission:  23:04:47\n",
            "View your submissions: https://cds-iisc.talentsprint.com/notebook_submissions\n"
          ]
        }
      ],
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}